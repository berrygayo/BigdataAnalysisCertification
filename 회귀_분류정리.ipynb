{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "회귀_분류정리.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNy85BvF+4X3ucxfclfZWvM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berrygayo/BigdataAnalysisCertification/blob/main/%ED%9A%8C%EA%B7%80_%EB%B6%84%EB%A5%98%EC%A0%95%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7CGPYdlk8mu"
      },
      "source": [
        "# 회귀 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV6pBVuFlpQ6"
      },
      "source": [
        "# 제주빅데이터센터 데이터를 활용하여 효율적인 교통량 측정을 위한 날씨/유동인구 활용 교통량 추이 데이터 제공 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zqz3IovhlpSz"
      },
      "source": [
        "# 0050 start\n",
        "# 0148 fin \n",
        "# ==============\n",
        "#  preprocessing\n",
        "# 라벨인코딩(시도명,읍면도명),id 제거 \n",
        "# ==============\n",
        "\n",
        "import pandas as pd\n",
        "train_X = pd.read_csv(\"https://raw.githubusercontent.com/Datamanim/jeju/main/Jeju_trainX.csv\",encoding='euc-kr')\n",
        "train_y= pd.read_csv(\"https://raw.githubusercontent.com/Datamanim/jeju/main/Jeju_trainy.csv\",encoding='euc-kr')\n",
        "test_X= pd.read_csv(\"https://raw.githubusercontent.com/Datamanim/jeju/main/Jeju_testX.csv\",encoding='euc-kr')\n",
        "sub= pd.read_csv(\"https://raw.githubusercontent.com/Datamanim/jeju/main/subExample.csv\",encoding='euc-kr')\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor, AdaBoostRegressor \n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score,train_test_split\n",
        "from sklearn.pipeline import Pipeline, make_pipeline \n",
        "from  sklearn.preprocessing import LabelEncoder,MinMaxScaler, RobustScaler, StandardScaler\n",
        "\n",
        "#get_summies 확인필요 !!!!!!!!!!!!!!!!! \n",
        "train_X['시도명'] = LabelEncoder().fit_transform(train_X['시도명'])\n",
        "train_X['읍면동명'] = LabelEncoder().fit_transform(train_X['읍면동명'])\n",
        "test_X['시도명'] = LabelEncoder().fit_transform(test_X['시도명'])\n",
        "test_X['읍면동명'] =LabelEncoder().fit_transform(test_X['읍면동명'])\n",
        "\n",
        "# 월만 빼오기 \n",
        "train_X['일자'] = pd.to_datetime(train_X['일자'])\n",
        "train_X['월'] = train_X['일자'].dt.month\n",
        "test_X['일자'] = pd.to_datetime(test_X['일자'])\n",
        "test_X['월'] = test_X['일자'].dt.month\n",
        "\n",
        "train_X = train_X.drop(['id','일자'], axis=1)\n",
        "test_X = test_X.drop(['id','일자'], axis=1)\n",
        "train_y = train_y.drop('id', axis=1)\n",
        "\n",
        "sc = StandardScaler()\n",
        "train_X[['거주인구','근무인구','방문인구','총 유동인구']] = sc.fit_transform(train_X[['거주인구','근무인구','방문인구','총 유동인구']])\n",
        "test_X[['거주인구','근무인구','방문인구','총 유동인구']] = sc.fit_transform(test_X[['거주인구','근무인구','방문인구','총 유동인구']])\n",
        "\n",
        "# ================\n",
        "# 모델찾기 \n",
        "# ===============\n",
        "\n",
        "\"\"\"\n",
        "RF = RandomForestRegressor()\n",
        "GBR = GradientBoostingRegressor()\n",
        "BR = BaggingRegressor()\n",
        "ABR = AdaBoostRegressor()\n",
        "LR = LinearRegression()\n",
        "Lasso =  Lasso()\n",
        "Ridge = Ridge()\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.15)\n",
        "\n",
        "print('>>> Randomforest')\n",
        "RF.fit(X_train, y_train)\n",
        "print(RF.score(X_train, y_train))\n",
        "print(RF.score(X_val, y_val))\n",
        "\n",
        "print('>>> GradientBoostingRegressor')\n",
        "GBR.fit(X_train, y_train)\n",
        "print(GBR.score(X_train, y_train))\n",
        "print(GBR.score(X_val, y_val))\n",
        "\n",
        "print('>>> BaggingRegressor')\n",
        "BR.fit(X_train, y_train)\n",
        "print(BR.score(X_train, y_train))\n",
        "print(BR.score(X_val, y_val))\n",
        "\n",
        "print('>>> AdaBoostRegressor')\n",
        "ABR.fit(X_train, y_train)\n",
        "print(ABR.score(X_train, y_train))\n",
        "print(ABR.score(X_val, y_val))\n",
        "\n",
        "print('>>> LinearRegression')\n",
        "LR.fit(X_train, y_train)\n",
        "print(LR.score(X_train, y_train))\n",
        "print(LR.score(X_val, y_val))\n",
        "\n",
        "print('>>> Lasso')\n",
        "Lasso.fit(X_train, y_train)\n",
        "print(Lasso.score(X_train, y_train))\n",
        "print(Lasso.score(X_val, y_val))\n",
        "\n",
        "print('>>> Ridge')\n",
        "Ridge.fit(X_train, y_train)\n",
        "print(Ridge.score(X_train, y_train))\n",
        "print(Ridge.score(X_val, y_val))\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ================= \n",
        "# 하이퍼파라미터 튜닝 \n",
        "# =================\n",
        "\"\"\"\n",
        "RF = RandomForestRegressor()\n",
        "GBR = GradientBoostingRegressor()\n",
        "BR = BaggingRegressor()\n",
        "\n",
        "####### 일단 검정 셋으로 확인할거라 X_train 사용 > train_X로 변경 해야함 !!! \n",
        "print('>>> Randomforest')\n",
        "# {'max_depth': 7, 'n_estimators': 10} \n",
        "# 0.87\n",
        "\n",
        "param_grid = {'max_depth':[3,5,7],'n_estimators':[1,10,20]}\n",
        "gs = GridSearchCV(RF,param_grid=param_grid, cv=5, verbose=True)\n",
        "gs.fit(X_train, y_train)\n",
        "print(gs.best_params_)\n",
        "print(gs.best_score_)\n",
        "\n",
        "print('>>> GradientBoostingRegressor')\n",
        "#{'learning_rate': 0.1, 'n_estimators': 200}\n",
        "# 0.9488039085087507\n",
        "param_grid = {'learning_rate':[0.01,0.1,1],'n_estimators':[10,100,200]}\n",
        "gs = GridSearchCV(GBR,param_grid=param_grid, cv=5, verbose=True)\n",
        "gs.fit(X_train, y_train)\n",
        "print(gs.best_params_)\n",
        "print(gs.best_score_)\n",
        "\n",
        "print('>>> BaggingRegressor')\n",
        "#{'n_estimators': 20}\n",
        "#0.9674245574047695\n",
        "param_grid = {'n_estimators':[1,10,20]}\n",
        "gs = GridSearchCV(BR,param_grid=param_grid, cv=5, verbose=True)\n",
        "gs.fit(X_train, y_train)\n",
        "print(gs.best_params_)\n",
        "print(gs.best_score_)\n",
        "\"\"\"\n",
        "# ========= \n",
        "# 모델 생성 \n",
        "# ======== \n",
        "RF = RandomForestRegressor(max_depth= 7, n_estimators= 10)\n",
        "RF.fit(train_X, train_y)\n",
        "pred_RF = RF.predict(test_X)\n",
        "\n",
        "GBR = GradientBoostingRegressor(learning_rate= 0.1, n_estimators= 200)\n",
        "GBR.fit(train_X, train_y)\n",
        "pred_GBR = GBR.predict(test_X)\n",
        "\n",
        "BR = BaggingRegressor(n_estimators=20)\n",
        "BR.fit(train_X, train_y)\n",
        "pred_BR = BR.predict(test_X)\n",
        "\n",
        "pred_y = (pred_RF+pred_GBR+pred_BR)/3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDMd7TE2lnZc"
      },
      "source": [
        "# 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXUDtlbwlpv-"
      },
      "source": [
        "# 3:20 start \n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.pipeline import Pipeline, make_pipeline \n",
        "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler, MinMaxScaler \n",
        "from sklearn.metrics import accuracy_score, make_scorer, roc_auc_score\n",
        "pd.set_option('display.max_columns',500)\n",
        "\n",
        "X_train = pd.read_csv('data/X_train.csv')\n",
        "X_test = pd.read_csv('data/X_test.csv')\n",
        "y_train = pd.read_csv('data/y_train.csv')\n",
        "\n",
        "# ==================\n",
        "# preprocessing X_train, X_test 동일하게 해야함 ! \n",
        "# 1. 결측값 확인, 최대최솟값 확인, 라벨인코딩, 마지막에 cust_id 제거 \n",
        "# X_train : 3500개, X_test :2500개 \n",
        "# ==================== \n",
        "X_train = X_train.fillna(0)\n",
        "X_test = X_test.fillna(0)\n",
        "\n",
        "X_train['주구매상품'] = pd.get_dummies(X_train['주구매상품'])\n",
        "X_train['주구매지점'] = pd.get_dummies(X_train['주구매지점'])\n",
        "X_test['주구매상품'] = pd.get_dummies(X_test['주구매상품'])\n",
        "X_test['주구매지점'] = pd.get_dummies(X_test['주구매지점'])\n",
        "\n",
        "X_train = X_train.drop('cust_id', axis=1)\n",
        "X_test = X_test.drop('cust_id', axis=1)\n",
        "y_train = y_train.drop('cust_id', axis=1)\n",
        "\n",
        "# ================\n",
        "# 모델선택 \n",
        "# ===============\n",
        "LR = Pipeline([('sc',RobustScaler()),('model',LogisticRegression())])#**************수케일러,모델 뒤 소괄호 \n",
        "RF = Pipeline([('sc',RobustScaler()),('model',RandomForestClassifier())])\n",
        "Bag = Pipeline([('sc',RobustScaler()),('model',BaggingClassifier())])\n",
        "GBR = Pipeline([('sc',RobustScaler()),('model',GradientBoostingClassifier())])\n",
        "\n",
        "print(\"LR : \", cross_val_score(LR,X_train,y_train,cv=5,verbose=True).mean()) #0.634\n",
        "print(\"RF : \", cross_val_score(RF,X_train,y_train,cv=5,verbose=True).mean()) \n",
        "print(\"Bag : \", cross_val_score(Bag,X_train,y_train,cv=5,verbose=True).mean())\n",
        "print(\"GBR : \", cross_val_score(GBR,X_train,y_train,cv=5,verbose=True).mean()) #0.631\n",
        "\n",
        "# =========== \n",
        "# 하이퍼파라미터튜닝 \n",
        "# ========== \n",
        "score = make_scorer(roc_auc_score)\n",
        "# 랜덤 포레스트 : {4,10}\n",
        "RF = Pipeline([('sc',RobustScaler()),('model',RandomForestClassifier())])\n",
        "param_grid = {'model__max_depth':[2,3,4], 'model__n_estimators':[10,100,200]} # ****model__ 붙여야함 !!!  \n",
        "gs = GridSearchCV(RF,param_grid,scoring=score,cv=5)\n",
        "gs.fit(X_train,y_train)\n",
        "print(gs.best_params_)\n",
        "print(gs.best_score_)\n",
        "\n",
        "# GradientBoosting {0.1,100}\n",
        "GBR = Pipeline([('sc',RobustScaler()),('model',GradientBoostingClassifier())])\n",
        "param_grid = {'model__learning_rate':[0.01,0.1,1], 'model__n_estimators':[10,100,200]} # ****model__ 붙여야함 !!!  \n",
        "gs = GridSearchCV(GBR,param_grid,scoring=score,cv=5)\n",
        "gs.fit(X_train,y_train)\n",
        "print(gs.best_params_)\n",
        "print(gs.best_score_)\n",
        "\n",
        "# Logistic {1}\n",
        "LR = Pipeline([('sc',RobustScaler()),('model',LogisticRegression())])\n",
        "param_grid = {'model__C':[0.1,1,10]} # ****model__ 붙여야함 !!!  \n",
        "gs = GridSearchCV(LR,param_grid,scoring=score,cv=5)\n",
        "gs.fit(X_train,y_train)\n",
        "print(gs.best_params_)\n",
        "print(gs.best_score_)\n",
        "\n",
        "# ==========\n",
        "# 모델 적용 \n",
        "# =========== \n",
        "LR = LogisticRegression()\n",
        "LR.fit(X_train,y_train)\n",
        "pred_LR = LR.predict_proba(X_test)\n",
        "\n",
        "GBR = GradientBoostingClassifier()\n",
        "GBR.fit(X_train,y_train)\n",
        "pred_GBR = GBR.predict_proba(X_test)\n",
        "\n",
        "RF = RandomForestClassifier(max_depth=4, n_estimators=10)\n",
        "RF.fit(X_train,y_train)\n",
        "pred_RF = RF.predict_proba(X_test)\n",
        "\n",
        "pred_y = (pred_LR + pred_RF + pred_GBR)/3\n",
        "pred_Y= pd.DataFrame({'gender':pred_y[:,1]})\n",
        "\n",
        "X_test_2 = pd.read_csv('data/X_test.csv')\n",
        "res=pd.concat([X_test_2[['cust_id']], pred_Y],axis=1)\n",
        "\n",
        "res.to_csv('14010353.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RrsUR_glp0N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lur8UedUlp2G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7AWQb1Ilp4f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
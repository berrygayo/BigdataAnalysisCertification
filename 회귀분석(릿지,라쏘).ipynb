{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " 회귀분석(릿지,라쏘).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNfi2lQzsQd9YGbGs2kcAfe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berrygayo/BigdataAnalysisCertification/blob/main/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D(%EB%A6%BF%EC%A7%80%2C%EB%9D%BC%EC%8F%98).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLTR7tKsfI9E"
      },
      "source": [
        "## 선형 회귀 (최소 제곱법)\n",
        "+ 평균제곱오차를 최소화하는 파라미터 w,b를 찾는다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_spy4tTI2-3Y"
      },
      "source": [
        "#!pip install mglearn\n",
        "import mglearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOcZyeBrfFCZ"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression \n",
        "from sklearn.model_selection import train_test_split\n",
        "X, y  = mglearn.datasets.make_wave(n_samples=60)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "lr = LinearRegression().fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rmSYoAxi6Na"
      },
      "source": [
        "print(\"lr.coef_:\", lr.coef_) # w\n",
        "print(\"lr.intercept_:\", lr.intercept_) # b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClCivlCxjLJD"
      },
      "source": [
        "print(\"훈련 세트 점수: {:.2f}\".format(lr.score(X_train, y_train)))\n",
        "print(\"테스트 세트 점수: {:.2f}\".format(lr.score(X_test, y_test)))\n",
        "\n",
        "# R^2 값이 그닥 높지 않다.\n",
        "# 훈련세트와 테스트세트의 점수가 비슷 : 과대적합이 아니라 과소 적합인 상태를 뜻함 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-tpvr8wfCVl"
      },
      "source": [
        "### 그렇다면 선형회귀모델이 보스턴 주택 가격과 같은 복잡한 데이테에서는 어떻게 적용하는가 ? \n",
        "( 샘플 506개, 특성은 유도된 것을 합쳐 104개 )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Ei7Lmjd3I5"
      },
      "source": [
        "X, y = mglearn.datasets.load_extended_boston()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "lr = LinearRegression().fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qlvp0KZd3LP"
      },
      "source": [
        "print(\"훈련 세트 점수: {:.2f}\".format(lr.score(X_train, y_train)))\n",
        "print(\"테스트 세트 점수: {:.2f}\".format(lr.score(X_test, y_test)))\n",
        "\n",
        "# 훈련 세트에 과적합됨 > 억제 필요 > 릿지나 라쏘 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhdjk-RHd3Nn"
      },
      "source": [
        "## 릿지 회귀 \n",
        "+ L2 규제 (L2 norm)\n",
        "+ 가중치 파라미터(w)를 0에 가깝게\n",
        "+ 유클리드 사용 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdEG-arJRtaW"
      },
      "source": [
        "from sklearn.linear_model import Ridge \n",
        "ridge = Ridge().fit(X_train, y_train) # alpha 지정 안할 시 기본 1로 설정됨 \n",
        "print(\"훈련 세트 점수 : {:.2f}\".format(ridge.score(X_train, y_train)))\n",
        "print(\"테스트 세트 점수: {:.2f}\".format(ridge.score(X_test,y_test)))\n",
        "\n",
        "# 기존 선형 모델의 테스트 점수 : 0.69 보다 높아짐 \n",
        "# 선형회귀는 테스트 셋에 과적합 되지만 릿지는 덜 그럼 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zr3DQa9SJjs"
      },
      "source": [
        "ridge10 = Ridge(alpha=10).fit(X_train, y_train) # alpha를 높이면 계수를 더 0에 가깝게 만듬 > 훈련세트의 성능 낮아지지만, 일반화에는 도움 \n",
        "print(\"훈련 세트 점수: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
        "print(\"테스트 세트 점수: {:.2f}\".format(ridge10.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z02lhntLSHqr"
      },
      "source": [
        "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
        "print(\"훈련 세트 점수: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
        "print(\"테스트 세트 점수: {:.2f}\".format(ridge01.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XssJTe87j9fq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# alpha 값 변경에 따른 coef_속성 변경 확인하기 \n",
        "plt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\n",
        "plt.plot(ridge.coef_, 's', label=\"Ridge alpha=1\")\n",
        "plt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\n",
        "\n",
        "plt.plot(lr.coef_, 'o', label=\"LinearRegression\")\n",
        "plt.xlabel(\"계수 목록\")\n",
        "plt.ylabel(\"계수 크기\")\n",
        "xlims = plt.xlim()\n",
        "plt.hlines(0, xlims[0], xlims[1])\n",
        "plt.xlim(xlims)\n",
        "plt.ylim(-25, 25)\n",
        "plt.legend()\n",
        "plt.show() # 책에는 없음"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dvk-4zATRJQ"
      },
      "source": [
        "> 그래프 해석 \n",
        "+ x 축은 coef_의 원소를 위치대로 나열한 것 \n",
        "+ x=0은 첫 번째 특성에 연관된 계수 \n",
        "+ x=1은 두 번째 특성에 연관된 계수, 이런 식으로 x=100까지 계속 된다. \n",
        "+ y 축은 각 계수의 수치, alpha=10 일 때 대부분의 계수는 -3과 3사이에 위치 \n",
        "+ alpha = 0.1 일 때 계수는 더 커지며 \n",
        "+ alpha=0 규제가 없는 선형 회귀의 계수는 값이 더 커져 그림 밖으로 넘어간다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXA30PvBd3Ps"
      },
      "source": [
        "## 학습 곡선 그리기 \n",
        "+ 데이터셋의 크기에 따른 모델의 성능 변화 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iwqbJPYd3Sk"
      },
      "source": [
        "mglearn.plots.plot_ridge_n_samples()\n",
        "# 선형회귀랑 , alpha =1 일때 Ridge 적용 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs5gaSU_Va5Y"
      },
      "source": [
        "> 그래프 해석 \n",
        "+ 훈련 세트에 대해선 두 모델 다 처음부터 점수가 높다.\n",
        "+ 훈련 세트에서 선형 회귀 덤수가 리지보다 높은데, 이는 규제가 없기 때문 \n",
        "+ 테스트 세트에선 릿지가 선형회귀보다 점수가 높으며, 선형의 경우 n=400 미만에선 아무것도 학습 못하고 있음 \n",
        "+ 두 모델 모두 데이터가 많이질수록 좋아지고 마지막에는 선형회귀가 릿지를 따라잡는다. \n",
        "+ 선형 회귀의 훈련 데이터는 성능이 감소 : 데이터가 많아질수록 모델이 데이터를 기억하거나 과대적합하기 어려워지기 때문 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlvGL317d3YE"
      },
      "source": [
        "## 라쏘 회귀 \n",
        "+ L1 규제 \n",
        "+ 가중치를 0으로 \n",
        "+ 모델에서 완전히 제외되는 특성이 생길 수 있다 \n",
        "+ 멘하탄 거리 사용 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv4eJihDaMdx"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yS7zy5fZ_Fa"
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "lasso = Lasso().fit(X_train, y_train)\n",
        "print(\"훈련 세트 점수: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
        "print(\"테스트 세트 점수: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
        "print(\"사용한 특성의 개수:\", np.sum(lasso.coef_ != 0))\n",
        "\n",
        "# 104개 특성중 4개만 사용 \n",
        "# 훈련세트와 테스트 세트 모두 점수가 낮음 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGWCye3HanMb"
      },
      "source": [
        "# 과소적합을 줄이기 위해 alpha 값을 줄여보자 \n",
        "# 이렇게 하려면 max_iter(반복 실행하는 최대 횟수)의 기본값을 늘려야 한다. \n",
        "\n",
        "# max_iter 기본 값을 증가시키지 않으면 max_iter 값을 늘이라는 경고가 발생합니다\n",
        "lasso001 = Lasso(alpha=0.01, max_iter=50000).fit(X_train, y_train)\n",
        "print(\"훈련 세트 점수: {:.2f}\".format(lasso001.score(X_train, y_train)))\n",
        "print(\"테스트 세트 점수: {:.2f}\".format(lasso001.score(X_test, y_test)))\n",
        "print(\"사용한 특성의 개수:\", np.sum(lasso001.coef_ != 0))\n",
        "\n",
        "# 알파값을 낮추면 모델의 복잡도는 증가하여 성능 좋아짐 \n",
        "# 사용한 특성은 104개 중 33개 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF3YhQasX-Vs"
      },
      "source": [
        "lasso00001 = Lasso(alpha=0.0001, max_iter=50000).fit(X_train, y_train)\n",
        "print(\"훈련 세트 점수: {:.2f}\".format(lasso00001.score(X_train, y_train)))\n",
        "print(\"테스트 세트 점수: {:.2f}\".format(lasso00001.score(X_test, y_test)))\n",
        "print(\"사용한 특성의 개수:\", np.sum(lasso00001.coef_ != 0))\n",
        "# 알파값을 너무 낮추면 과대적합 발생 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjL92TpUcSwM"
      },
      "source": [
        "# 알파값의 따른 모델의 계수 그래프 \n",
        "plt.plot(lasso.coef_, 's', label=\"Lasso alpha=1\")\n",
        "plt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")\n",
        "plt.plot(lasso00001.coef_, 'v', label=\"Lasso alpha=0.0001\")\n",
        "\n",
        "plt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\")\n",
        "plt.legend(ncol=2, loc=(0, 1.05))\n",
        "plt.ylim(-25, 25)\n",
        "plt.xlabel(\"계수 목록\")\n",
        "plt.ylabel(\"계수 크기\")\n",
        "plt.show() # 책에는 없음"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqUWSThWcS0v"
      },
      "source": [
        "> 그래프 해석 \n",
        "+ alpha =1 일때 계수 대부분이 0이고, 나머지 계수들도 크기가 작다 \n",
        "+ alpha =0.01 로 줄이면 대부분의 특성이 0이 되는 분포를 얻게 된다.\n",
        "+ alpha=0.0001이 되면 계수 대부분이 0이 아니고 값도 커져 꽤 규제받지 않는 모델이 된다\n",
        "\n",
        "+ alpha=0.1 인 릿지 모델은 alpha=0.01인 라소 모델과 성능이 비슷하지만 릿지를 사용하면 어느 계수도 0이 되지 않는다,.\n",
        "\n",
        "\n",
        "실제로 이 두 모델 중 보통은 리지 회귀를 선호한다, 하지만 특성이 많고 그중 일부분만 중요하다면\n",
        "라쏘가 더 좋은 선택일 수 있다.\n",
        "또한 분석하기 쉬운 모델을 원한다면 라쏘가 입력 특성 중 일부만 사용하므로 쉽게 해석할 수 있는 모델을 만들어 줄 것 \n",
        "사이키런은 라쏘와 릿지의 패널티를 결합한 엘라스틱넷도 제공 \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAGEATPQcS3z"
      },
      "source": [
        "# 분류에 대한 선형 모델 \n",
        "+ 함수값 < 0 : -1, 함수값 > 0 : 1 로 예측 \n",
        "\n",
        "> 두 가지 방법으로 구분 \n",
        "1. 특정 계수와 절편의 조합이 훈련 데이터에 얼마나 잘 맞는지 \n",
        "2. 사용할 수 있는 규제가 있는지, 있다면 어떤 방식인지 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqCcurCcyUVT"
      },
      "source": [
        "## 로지스틱 회귀(LogisticRegression) & 서보트 벡터 머신 (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiHATBkRcS6g"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "X, y = mglearn.datasets.make_forge()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
        "\n",
        "for model, ax in zip([LinearSVC(max_iter=5000), LogisticRegression()], axes):\n",
        "    clf = model.fit(X, y)\n",
        "    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n",
        "                                    ax=ax, alpha=.7)\n",
        "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
        "    ax.set_title(clf.__class__.__name__)\n",
        "    ax.set_xlabel(\"특성 0\")\n",
        "    ax.set_ylabel(\"특성 1\")\n",
        "axes[0].legend()\n",
        "plt.show() # 책에는 없음"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpZUJmrNcS9O"
      },
      "source": [
        "> 그림 해석 \n",
        "+ 첫 번째 특성을 x축, 두 번쨰 특성을 y축에 둠 \n",
        "+ 직선 위는 1, 아래는 0 클래스로 분류 \n",
        "+ 두 모델 다 비슷하게 분류하였고, 똑같이 두 포인트를 잘못 분류 \n",
        "+ 이 두 모델 모두 L2 규제 사용 \n",
        "+ 두 모델 모두 규제의 강도를 결정하는 매개변수는 C 이다. \n",
        "+ C 값이 높아지면 규제 감소 \n",
        "+ C 값이 낮아지면 데이터 포인트 주 ㅇ다수에 맞추려고 하고, C 값을 높이면 개개의 데이터 포인트를 정확히 분류할 것 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvZminRXz6uK"
      },
      "source": [
        "mglearn.plots.plot_linear_svc_regularization()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNeZEnVvz7Bx"
      },
      "source": [
        "> 그래프 해석 \n",
        "+ 왼쪽 그림은 아주 작은 C 값 때문에 규제가 많이 적용 \n",
        "+ 중간 그림은 C값이 조금 더 크며 잘못 분류한 두 샘플에 민감해져 결정 경계가 기울어졌습니다. \n",
        "+ 오른쪽 그림에서 C값을 아주 크게 하였더니 결정 경계는 더 기울었고 마침내 클래스 0의 모든 데이터 포인트를 올바로 분류 했다.\n",
        "+ 오른쪽 그림의 모델은 모든 데이터 포인트를 정확하게 분류하려고 애썼지만 클래스의 전체적인 배치를 잘 파악하지 못했다.( 과대적합 ) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLpHMTEu01ax"
      },
      "source": [
        "분류에서의 선형 모델은 낮은 차원의 데이터에서는 결정 경계가 직선이거나 평면이어서 매우 제한적인 것 처럼 보인다.\n",
        "하지만 고차원에서는 분류에 대한 선형 모델이 매우 강력해지며 특성이 많아지면 과대적합되지 않도록 하는 것이 매우 중요해진다.\n",
        "특성이 맣아지면 과대적합되지 않도록 하는 것이 매우 중요해진다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8mwRNRk01c3"
      },
      "source": [
        "## 유방암 데이터 셋을 사용한 LogisticRegression "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBJCgzvZ01fD"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "cancer = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
        "logreg = LogisticRegression(max_iter=5000).fit(X_train, y_train)\n",
        "print(\"훈련 세트 점수: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
        "print(\"테스트 세트 점수: {:.3f}\".format(logreg.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdObsUMN1Nrx"
      },
      "source": [
        "기본값 C=1 이 훈련 세트와 테스트 세트 양쪽에 95% 정확도로 꽤 훌륭한 성능을 내고 있다.\n",
        "하지만 두 성능이 매우 비슷하므로 과소적합인 것 같다.\n",
        "모델의 제약을 더 풀어주기 위해 C 를 증가시켜본다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "matljusk1gsJ"
      },
      "source": [
        "logreg100 = LogisticRegression(C=100, max_iter=5000).fit(X_train, y_train)\n",
        "print(\"훈련 세트 점수: {:.3f}\".format(logreg100.score(X_train, y_train)))\n",
        "print(\"테스트 세트 점수: {:.3f}\".format(logreg100.score(X_test, y_test)))\n",
        "\n",
        "# 훈련, 테스트 둘 다 증가 \n",
        "# 이는 복잡도가 높은 모델일수록 성능이 좋음을 말해준다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDsVP0P-1gyB"
      },
      "source": [
        "logreg001 = LogisticRegression(C=0.01, max_iter=5000).fit(X_train, y_train)\n",
        "print(\"훈련 세트 점수: {:.3f}\".format(logreg001.score(X_train, y_train)))\n",
        "print(\"테스트 세트 점수: {:.3f}\".format(logreg001.score(X_test, y_test)))\n",
        "\n",
        "# 규제를 더 강하게 하기 위해 C=0.01 사용 \n",
        "# 과소적합된 모델, 정확도 낮아짐 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm9R-0ZC1g0f"
      },
      "source": [
        "# C 설정에 따른 모델의 계수 확인 \n",
        "plt.plot(logreg100.coef_.T, '^', label=\"C=100\")\n",
        "plt.plot(logreg.coef_.T, 'o', label=\"C=1\")\n",
        "plt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\n",
        "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
        "xlims = plt.xlim()\n",
        "plt.hlines(0, xlims[0], xlims[1])\n",
        "plt.xlim(xlims)\n",
        "plt.ylim(-5, 5)\n",
        "plt.xlabel(\"특성\")\n",
        "plt.ylabel(\"계수 크기\")\n",
        "plt.legend()\n",
        "plt.show() # 책에는 없음"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u8DDFJ_1g3T"
      },
      "source": [
        "> 그래프 해석 \n",
        "+ 규제를 강하게 할수록 계수들을 0에 더 가깝게 만들지만 완전히 0이 되지는 않는다.\n",
        "+ 세 번째 계수(mean perimeter)를 보면 C=100, C=1일때 이 계수는 음수지만, C=0.001일때 양수가 되며 C=1일때보다 절대값이 크다.\n",
        "+ texture error 특성은 악성인 샘플과 관련이 깊다, 하지만 mean periment는 계수가 바뀌는 것을 보아 양성이나 악성의 신호 모두가 될 수 있다. \n",
        "\n",
        "더 이해하기 쉬운 모델을 원한다면 \n",
        "L1 규제를 사용하는 것이 좋다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZVga03h1g5O"
      },
      "source": [
        "### L1 규제 사용 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4_q0x7i3RfL"
      },
      "source": [
        "for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n",
        "    lr_l1 = LogisticRegression(solver='liblinear', C=C, penalty=\"l1\", max_iter=1000).fit(X_train, y_train)\n",
        "    # penalty : 어떤 규제를 사용할 지 결정 \n",
        "    # L1 : 모든 규제 사용 x , 계수가 0이 될 수 있음/. \n",
        "    print(\"C={:.3f} 인 l1 로지스틱 회귀의 훈련 정확도: {:.2f}\".format(\n",
        "          C, lr_l1.score(X_train, y_train)))\n",
        "    print(\"C={:.3f} 인 l1 로지스틱 회귀의 테스트 정확도: {:.2f}\".format(\n",
        "          C, lr_l1.score(X_test, y_test)))\n",
        "    plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\n",
        "\n",
        "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
        "xlims = plt.xlim()\n",
        "plt.hlines(0, xlims[0], xlims[1])\n",
        "plt.xlim(xlims)\n",
        "plt.xlabel(\"특성\")\n",
        "plt.ylabel(\"계수 크기\")\n",
        "\n",
        "plt.ylim(-5, 5)\n",
        "plt.legend(loc=3)\n",
        "plt.show() # 책에는 없음"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}